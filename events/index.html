<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="crT26-QqFksmh-GiQYWgoAoNEhJdto58BzD3VJBNdWQ"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Events | WeLight</title> <meta name="author" content="Evan Y. Peng"> <meta name="description" content="Computational Imaging &amp; Mixed Representation Labortaty "> <meta name="keywords" content="Computational Imaging, Mixed Representation Labortaty"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?c4de7ed5661f0ca67e065788717516a0"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?c0069978495cdb6a4c56ca27eda22252"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hku.welight.fun/events/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">WeLight</span> @ HKU</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item active"> <a class="nav-link" href="/events/">Events<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">PI</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <title>events</title> <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700;900&amp;display=swap" rel="stylesheet"> <script src="https://cdn.tailwindcss.com"></script> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css"> <style>.h1_1{font-size:40px!important;font-weight:900!important}.h2_1{color:#007bff!important;font-size:51px!important;font-weight:500!important;margin-top:-2.5rem!important;margin-bottom:4rem!important}.textarea{color:#FFF!important;font-weight:600!important;font-size:larger!important}.textareahead{color:#FFF!important;font-size:larger!important;font-size:30px!important;font-weight:700!important}.conference-background{display:flex;flex-direction:column;justify-content:center;align-items:start;background-image:url('/assets/img/bg_3.jpg');background-size:cover;background-attachment:fixed!important;background-position:center center;padding-top:5rem!important;padding-bottom:5rem!important;height:17rem;position:relative;margin-top:1.5rem;width:100vw;margin-left:-50vw;left:50%}.text-container{position:absolute;top:50%;left:calc(50% - min(40%,39rem));transform:translateY(-50%);width:auto}.header-container{display:flex;align-items:stretch;flex-wrap:wrap}.title-container{display:flex;flex-direction:column;justify-content:center;margin-bottom:-40px;max-width:917px!important}.image-container{position:relative;overflow:hidden;margin-left:auto;order:2;align-items:center;display:flex;padding-top:39px}.conference-image{max-width:100%;max-height:100px;object-fit:cover}@media(max-width:1536px){.image-container{margin-left:-5px;order:1;margin-bottom:-2rem;margin-top:-2rem}.title-container{order:2;width:100%}.conference-image{max-width:400px;max-height:120px;object-fit:cover}}body{font-family:'Poppins',sans-serif!important}.schedule{width:100%;margin:0;background:rgba(0,123,255,0.04);border-radius:0;overflow:hidden;box-shadow:rgba(0,123,255,0.4) 5px 5px,rgba(0,123,255,0.3) 10px 10px,rgba(0,123,255,0.2) 15px 15px,rgba(0,123,255,0.1) 20px 20px,rgba(0,123,255,0.05) 25px 25px;margin-bottom:4rem;padding:1rem}.session{display:flex;align-items:center;border-bottom:0 solid #eee;padding-left:1rem;padding-top:10px;padding-bottom:10px}.time{display:flex;justify-content:center;align-items:center;flex:0 0 20%;font-weight:600;color:#007bff;margin-right:1rem}.details{flex:1}.title{font-size:18px;margin-bottom:5px;font-weight:600}.speaker{font-size:16px;color:#666}.speaker-container{display:flex;flex-wrap:wrap}.speaker-profile{align-items:flex-start;gap:1rem;margin-bottom:2rem}.speaker-photo{width:12rem;object-fit:cover;min-height:10rem;max-height:13rem;margin-right:2rem;margin-top:7px;margin-bottom:1rem;float:left}.speaker-content{flex:1}.speaker-name{font-weight:bold;font-size:larger}.speaker-title{font-size:16px;color:#666}.speaker-link a{color:#007bff;text-decoration:none}.speaker-abstract{font-style:normal}.speaker-bio{font-weight:normal}.conference-materials{margin:2rem auto}.material-detail{background-color:rgba(0,123,255,0.04);margin-bottom:1rem;transition:all .3s ease-in-out}.material-summary{font-weight:600;color:#007bff;padding:1rem;margin:0;cursor:pointer;outline:0;position:relative;list-style:none;transition:all .3s ease-in-out}.material-summary::after{content:'▼';position:absolute;right:1rem;transition:transform .3s ease-in-out}.material-detail[open] .material-summary::after{transform:rotate(180deg)}.material-content{padding:1rem;line-height:1.6;max-height:0;overflow:overlay;transition:max-height .3s ease-in-out}.material-content b{font-weight:600}s .material-content ol{padding-left:10px;list-style-type:none;margin-top:1rem}.material-content a{color:#007bff;text-decoration:none}.material-content ol li{position:relative;padding-left:20px;margin-bottom:10px;margin-top:1rem}.material-content ol li::before{content:"•";color:#007bff;font-weight:bold;position:absolute;left:0;top:0;font-size:20px}.material-content-name{font-weight:600;font-size:initial;margin-top:-1rem;margin-bottom:1rem}.material-detail[open] .material-content{max-height:30rem}.material-summary:hover{background-color:var(--global-hover-colorr)}details>summary{list-style:none}details>summary::-webkit-details-marker{display:none}.logos-container{display:flex;justify-content:center;align-items:center;gap:20px;margin-top:1rem;flex-wrap:wrap}.logos-container img{max-height:min(7rem,18vw);width:auto;transition:transform .3s ease}.logos-container img:hover{transform:scale(1.1)}.special-logo-1{max-width:max(5rem,7vw)}.special-logo-2{max-width:max(6rem,8vw)}.foot-title{font-size:24px;color:#666;font-weight:700}.foot-link a{color:#007bff;text-decoration:none;font-size:24px;font-weight:300}.foot-title b{font-size:32px;font-weight:700}.register-button{display:inline-block;color:white!important;padding:10px 20px;text-align:center;text-decoration:none!important;font-size:21px;transition:all .3s ease;position:relative;overflow:hidden;margin-bottom:1rem;border-radius:5px;background:rgba(0,123,255,0.6)}.register-button::after{content:'→';position:absolute;right:-20px;top:50%;transform:translateY(-50%);transition:right .3s ease}.register-button:hover::after{right:10px}
.register-button:hover{padding-right:40px;box-shadow:rgba(0,123,255,0.4) 5px 5px,rgba(0,123,255,0.3) 10px 10px,rgba(0,123,255,0.2) 15px 15px,rgba(0,123,255,0.1) 20px 20px,rgba(0,123,255,0.05) 25px 25px}</style> <div class="container mx-auto px-4"> <div class="header-container"> <div class="title-container"> <h1 class="h1_1 text-6xl font-bold mt-10 mb-5 "> Workshop on</h1> <h2 class="h2_1 text-5xl font-bold mb-5">Frontiers of Image Science and Visual Computing 2024</h2> </div> <div class="image-container"> <img src="/assets/img/logos/Eng_.jpg" alt="HKU" class="conference-image"> </div> </div> <div class="bg-gray-200 p-5 mb-5 conference-background"> <div class="text-container"> <h3 class="text-3xl font-bold mb-2 textareahead">Let there be light</h3> <p class="mb-2 textarea">8 April, 2024 PM</p> <p class="mb-2 textarea">Rayson Huang Lecture Theatre, The University of Hong Kong, HONG KONG SAR</p> </div> </div> <h2 class="text-4xl font-bold mb-5">Workshop Objectives</h2> <div class="grid grid-cols-1 md:grid-cols-2 mb-10"> <div> <div class="flex items-start mb-2" style="margin-bottom: 2rem !important"> <div class="text-red-500 mr-2"> <i class="fas fa-circle"></i> </div> <div> <h4 class="font-bold">Encourage Innovative Spirit</h4> </div> </div> <div class="flex items-start mb-2" style="margin-bottom: 2rem !important"> <div class="text-red-500 mr-2"> <i class="fas fa-circle"></i> </div> <div> <h4 class="font-bold ">Promote Excellence and Sustain Quality</h4> </div> </div> </div> <div> <div class="flex items-start mb-2" style="margin-bottom: 2rem !important"> <div class="text-red-500 mr-2"> <i class="fas fa-circle"></i> </div> <div> <h4 class="font-bold ">Strive for Improvement</h4> </div> </div> <div class="flex items-start" style="margin-bottom: 2rem !important"> <div class="text-red-500 mr-2"> <i class="fas fa-circle"></i> </div> <div> <h4 class="font-bold ">Connect Communities</h4> </div> </div> </div> </div> <h2 class="text-4xl font-bold mb-5" style="margin-top: -2rem !important">Schedule</h2> <a href="https://hkuems1.hku.hk/hkuems/ec_hdetail.aspx?guest=Y&amp;ueid=93119" target="_blank" class="register-button" rel="external nofollow noopener">Register Now (Free)</a> <div class="schedule"> <div class="session"> <div class="time"></div> <div class="details"> <div class="title" style="color: #007BFF !important"><a href="https://www.eee.hku.hk/~evanpeng/" target="_blank" rel="external nofollow noopener">MC: Evan Y. Peng, The University of Hong Kong (HKU)</a></div> </div> </div> <div class="session"> <div class="time">14:15 - 14:30</div> <div class="details"> <div class="title">Check-in &amp; Welcome Preview</div> </div> </div> <div class="session"> <div class="time">14:30 - 14:40</div> <div class="details"> <div class="title">Opening Remarks</div> <div class="speaker">David Srolovitz, NAS, Dean of Engineering, HKU <br>Xu Liu, Director of SKL of Extreme Photonics and Instrumentation, Zhejiang University</div> </div> </div> <div class="session"> <div class="time">14:40 - 15:30</div> <div class="details"> <div class="title">William Mong Distinguished Lecture: Simulation Technologies for Image Systems</div> <div class="speaker">Brian A. Wandell, NAS, Stanford University</div> </div> </div> <div class="session"> <div class="time">15:30 - 15:35</div> <div class="details"> <div class="title">Pitching for Stanford Center for Image Systems Engineering (SCIEN)</div> <div class="speaker">Joyce Farrell, Executive Director of SCIEN</div> </div> </div> <div class="session"> <div class="time">15:35 - 16:05</div> <div class="details"> <div class="title">Tea Reception</div> </div> </div> <div class="session"> <div class="time">16:05 - 16:30</div> <div class="details"> <div class="title">Self-Calibrating, Fully Differentiable NLOS Inverse Rendering</div> <div class="speaker">Min H. Kim, Korea Advanced Institute of Science and Technology (KAIST)</div> </div> </div> <div class="session"> <div class="time">16:30 - 16:55</div> <div class="details"> <div class="title">High-speed Photometric Analysis using an Event Camera</div> <div class="speaker">Boxin Shi, Peking University (PKU)</div> </div> </div> <div class="session"> <div class="time">16:55 - 17:15</div> <div class="details"> <div class="title">When No-Reference Image Quality Models Meet MAP Estimation</div> <div class="speaker">Kede Ma, City University of Hong Kong (CityU)</div> </div> </div> <div class="session"> <div class="time">17:15 - 17:35</div> <div class="details"> <div class="title">Reshaping the Soundscape with User Defined Metamaterials for Imaging</div> <div class="speaker">Nicholas X. Fang, HKU</div> </div> </div> <div class="session"> <div class="time">17:35 - 17:40</div> <div class="details"> <div class="title">Closing Remarks</div> <div class="speaker">Head of EEE Department, HKU</div> </div> </div> </div> <h2 class="text-4xl font-bold mb-5">Speakers/Guests</h2> <div class="speaker-container"> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Brian%20A.%20Wandell.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Brian A. Wandell</h2> <h3 class="speaker-title">Founding Director, Stanford’s Center for Neurobiological Imaging, Stanford University</h3> <p class="speaker-link"><a href="https://wandell.vista.su.domains/blog/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract">Brian A. Wandell, the Isaac and Madeline Stein Family Professor, joined the Stanford Psychology faculty in 1979. He is a member, by courtesy, of Electrical Engineering, Ophthalmology, and the Graduate School of Education. Wandell is the Founding Director of Stanford’s Center for Cognitive and Neurobiological Imaging, and he served as a Deputy Director of the Wu Tsai Neuroscience Institute from 2013-2021. His research centers on vision science, spanning visual neuroscience to digital imaging systems (cameras, displays). He was elected to the American Academy of Arts and Sciences in 2011, received the highest honor of the Society for Imaging Science and Technology in 2014, was awarded the George A. Miller prize of the Cognitive Neuroscience Society in 2016, and Proctor Medal from the Association for Research in Vision and Ophthalmology in 2021. Wandell was elected to the US National Academy of Sciences in 2003.</p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/JoyceFarrell.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Joyce Farrell</h2> <h3 class="speaker-title">Executive Director, Stanford Center for Image Systems Engineering, Stanford University</h3> <p class="speaker-link"><a href="https://web.stanford.edu/group/scien/cgi-bin/farrell/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract">Joyce Farrell is the Executive Director of the Stanford Center for Image Systems Engineering. She has more than 20 years of research and professional experience working at a variety of companies and institutions, including the NASA Ames Research Center, New York University, the Xerox Palo Alto Research Center, Hewlett Packard Laboratories and Shutterfly (a startup company specializing in online digital photo-finishing). She is also the CEO and founder of ImagEval Consulting, LLC.</p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/minhkim.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Min H. Kim</h2> <h3 class="speaker-title">Chair Professor, Korea Advanced Institute of Science and Technology (KAIST)</h3> <p class="speaker-link"><a href="https://vclab.kaist.ac.kr/minhkim/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract">Min H. Kim is a Professor of Computer Science at the KAIST School of Computing, where he directs the Visual Computing Lab (VCLAB). Prior to joining KAIST, he was a postdoctoral researcher at Yale University. He holds a Ph.D. in Computer Science from University College London (UCL). He has received numerous awards, including the SIGGRAPH Technical Paper Award Honorable Mention in 2022. His main research areas are computational imaging, computational photography, 3D imaging, BRDF acquisition, and 3D reconstruction. He has served as Technical Paper Chair for Eurographics 2022, Course Chair for SIGGRAPH Asia 2022, and on many computer graphics and computer vision program committees, including SIGGRAPH, CVPR, ICCV, and Eurographics. He has served as an Associate Editor for top CS journals, including ACM Transactions on Graphics (TOG) and IEEE Transactions on Visualization and Computer Graphics (TVCG). </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Liu_Xu.png" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Xu Liu</h2> <h3 class="speaker-title">Director of State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang University</h3> <p class="speaker-link"><a href="https://scholar.google.com.hk/citations?user=xlJtfO8AAAAJ&amp;hl=zh-CN&amp;oi=ao" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Xu Liu is a Distinguished Professor of Optical Science and Engineering at Zhejiang University. Serving as the Director of the State Key Laboratory of Extreme Photonics and Instrumentation, he holds fellowships with OPTICA and SPIE, and is the Secretary General of the Chinese Optical Society (COS). His research primarily focuses on optical thin films coatings and metasurfaces, optical imaging techniques—including super-resolution microscopy and computational imaging—and display technologies such as 3D light field displays, projection displays, and AR/VR.</p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/BoxinShi.png" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Boxin Shi</h2> <h3 class="speaker-title">Associate Professor, Peking University</h3> <p class="speaker-link"><a href="https://camera.pku.edu.cn" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract">Boxin Shi is currently a Boya Young Fellow Associate Professor (with tenure) and Research Professor at Peking University, where he leads the Camera Intelligence Lab. He has also been a Young Scientist at Beijing Academy of Artificial Intelligence. He received the PhD degree from the University of Tokyo in 2013. From 2013 to 2017, he did research at MIT Media Lab, Singapore University of Technology and Design, Nanyang Technological University, and National Institute of Advanced Industrial Science and Technology. His research interests are computational photography and computer vision. He received the Okawa Foundation Research Grant in 2021. He has served as an associate editor of TAPMI/IJCV and an area chair of CVPR/ICCV/ECCV. He is a Senior Member of the IEEE/CCF/CSIG, and a Distinguished Lecturer of APSIPA. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/KedeMa.jpeg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Kede Ma</h2> <h3 class="speaker-title">Assistant Professor, City University of Hong Kong</h3> <p class="speaker-link"><a href="https://kedema.org/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract">Kede Ma is an Assistant Professor with the Department of Computer Science at City University of Hong Kong (CityU). He received the B.E. degree from the University of Science and Technology of China (USTC) in 2012, the MASc. and Ph.D. degrees from the University of Waterloo, in 2014 and 2017, respectively. Prior to joining CityU, he was a Research Associate with Howard Hughes Medical Institute and New York University, from 2018 to 2019. Dr. Ma leads the Multimedia Analytics (MA) Laboratory, which is an interdisciplinary research group. We are interested in Computational Vision, Computational Photography, Multimedia Forensics and Security.</p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Fang.png" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Nicholas X. Fang</h2> <h3 class="speaker-title">Professor, The University of Hong Kong</h3> <p class="speaker-link"><a href="https://www.mech.hku.hk/academic-staff/fang-x" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Professor Nicholas X. Fang is currently a Professor of Mechanical Engineering at the University of Hong Kong. Professor Fang earned his B.S. and M.S. degrees in Physics from Nanjing University, China; and Ph.D. degree in Mechanical Engineering from the University of California at Los Angeles. He was on the faculty of Mechanical Engineering at M.I.T. from 2011 to 2022, and as assistant professor of Mechanical Engineering at the University of Illinois at Urbana-Champaign from 2004 to 2010. Professor Fang teaches and conducts research in the area of micro/nanotechnology. Professor Fang’s research programs have focused on the scalable manufacturing processes for wave functional materials. While the main efforts focus on new insights of design for advanced manufacturing of wave functional material and devices, his group also actively pursue the applications in the areas of energy conversion, communication, and biomedical imaging. His research also leads to over 16 patent applications on nano- and micro-fabrication, additive manufacturing, and imaging technologies with successful technology transfer to industry (e.g. Osram, BASF, Nissan) and startups. </p> </div> </div> </div> <div class="conference-materials"> <h2 class="text-4xl font-bold mb-5">Workshop Materials</h2> <details class="material-detail"> <summary class="material-summary">William Mong Distinguished Lecture: Simulation Technologies for Image Systems</summary> <div class="material-content"> <h2 class="material-content-name">Brian A. Wandell</h2> <p>The number and type of imaging systems has grown enormously over the last several decades; these systems are an essential component in mobile communication, medicine, automotive and drone applications. Imaging systems are also increasingly used with deep learning systems that require large amounts of training data. For these reasons software prototyping has become an essential tool for the design, evaluation and training of modern image systems. I will describe three closely related open-source and freely available image systems toolboxes - ISETCam, ISETBio, and ISET3d - that support design and evaluation of whole image systems. The presentation will include examples of how we model the three-dimensional scene spectral radiance, retinal encoding (physiological optics and cone sampling), and image systems hardware (multi-element lenses, image sensors). I will also provide two examples of how we used end-to-end image system simulation to aid understanding and design. </p> <br> <b>Related publications</b> <br> <ol> <li> <a href="https://arxiv.org/abs/2101.01843" rel="external nofollow noopener" target="_blank">ISETAuto: Detecting vehicles with depth and radiance information (2021)</a> . Zhenyi Liu, Joyece Farrell, Brian Wandell, IEEE Access <a href="https://ieeexplore.ieee.org/document/9369340" rel="external nofollow noopener" target="_blank"> ACCESS.2021.3063692 </a> </li> <li> <a href="http://arxiv.org/abs/1902.04258" rel="external nofollow noopener" target="_blank">A system for generating complex physically accurate sensor images for automotive applications (2019)</a>. Zhenyi Liu, Minghao Shen, Jiaqi Zhang, Shuangting Liu, Henryk Blasinski, Trisha Lian, Brian Wandell. IS&amp;T Electronic Imaging Conference, San Francisco.</li> <li> <a href="https://jov.arvojournals.org/article.aspx?articleid=2753752" rel="external nofollow noopener" target="_blank">Ray tracing 3D spectral scenes through human optics models (2019)</a>. Trish Lian, Kevin McKenzie, David Brainard, Nicolas Cottaris, Brian Wandell. Journal of Vision October 2019, Vol.19, 23. doi:<a href="https://doi.org/10.1167/19.12.23" rel="external nofollow noopener" target="_blank">https://doi.org/10.1167/19.12.23</a> </li> <li> <a href="https://jov.arvojournals.org/article.aspx?articleid=2770341" rel="external nofollow noopener" target="_blank">A computational observer model of spatial contrast sensitivity: Effects of photocurrent encoding, fixational eye movements and inference engine (2020)</a>. Nicolas P. Cottaris, Brian A. Wandell, Fred Rieke, David H. Brainard Journal of Vision doi:<a href="https://doi.org/10.1167/jov.20.7.17" rel="external nofollow noopener" target="_blank">https://doi.org/10.1167/jov.20.7.17</a> </li> </ol> <p>See the wiki pages of the repositories at: <a href="https://github.com/ISET" rel="external nofollow noopener" target="_blank">https://github.com/ISET</a></p> </div> </details> <details class="material-detail"> <summary class="material-summary">Self-Calibrating, Fully Differentiable NLOS Inverse Rendering</summary> <div class="material-content"> <h2 class="material-content-name">Min H. Kim</h2> <p> Existing time-resolved non-line-of-sight (NLOS) imaging methods reconstruct hidden scenes by inverting the optical paths of indirect illumination measured at visible relay surfaces. These methods are prone to reconstruction artifacts due to inversion ambiguities and capture noise, which are typically mitigated through the manual selection of filtering functions and parameters. This talk introduces a fully-differentiable end-to-end NLOS inverse rendering pipeline that self-calibrates the imaging parameters during the reconstruction of hidden scenes, using as input only the measured illumination while working both in the time and frequency domains. The proposed pipeline extracts a geometric representation of the hidden scene from NLOS volumetric intensities and estimates the time-resolved illumination at the relay wall produced by such geometric information using differentiable transient rendering. It then uses gradient descent to optimize imaging parameters by minimizing the error between our simulated time-resolved illumination and the measured illumination. This end-to-end differentiable pipeline couples diffraction-based volumetric NLOS reconstruction with path-space light transport and a simple ray marching technique to extract detailed, dense sets of surface points and normals of hidden scenes. This talk demonstrates the robustness of the proposed method to consistently reconstruct geometry and albedo, even under significant noise levels. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">High-speed Photometric Analysis using an Event Camera</summary> <div class="material-content"> <h2 class="material-content-name">Boxin Shi</h2> <p> Compared with conventional frame-based cameras, the event camera has unique advantages, especially in their ability to perceive high-speed moving objects and scenes with high dynamic range. Existing research has fully demonstrated the advantages of event cameras in computer vision tasks such as image deblurring, high dynamic range imaging, and high-speed object detection and recognition. However, the photometric image formation model of event cameras has not been fully analyzed. This talk will share a series of research progress on modeling and analyzing the photometric image formation model of an event camera that records and responses to high-speed radiance changes: obtaining high-fidelity scene radiance estimation via analyzing the transient event frequency, constructing real-time photometric stereo with fast-moving light sources to estimate surface normals, and conducting direct-global illumination separation by capturing shadows of a line occlude swiftly sweeping over the scene. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">When No-Reference Image Quality Models Meet MAP Estimation</summary> <div class="material-content"> <h2 class="material-content-name">Kede Ma</h2> <p> Contemporary no-reference image quality assessment (NR-IQA) models can effectively quantify the perceived image quality, with high correlations between model predictions and human perceptual scores on fixed test sets. However, little progress has been made in comparing NR-IQA models from a perceptual optimization perspective. However, little progress has been made in leveraging NR-IQA models as natural image priors for use in real-world image enhancement. Here, for the first time, we demonstrate that NR-IQA models can be plugged into the maximum a posteriori (MAP) estimation framework for image enhancement. This is achieved by taking the gradients differentiable and bijective diffusion latents rather than in the raw pixel domain. Different NR-IQA models are likely to induce different enhanced images, which are ultimately subject to psychophysical testing. This leads to a new computational method for comparing NR-IQA models within the analysis-by-synthesis framework. Compared to conventional correlation-based metrics, our method provides complementary insights into the relative strengths and weaknesses of the competing NR-IQA models in the context of perceptual optimization. Taking a step further, we finetune the best-performing NR-IQA model on the combination of previously trained datasets and newly MAP-enhanced images, showcasing the value of MAP-enhanced images for NR-IQA model rectification. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Reshaping the Soundscape with User Defined Metamaterials for Imaging</summary> <div class="material-content"> <h2 class="material-content-name">Nicholas X. Fang</h2> <p> TBD </p> </div> </details> </div> <footer style="text-align:center; padding:20px;"> <p class="foot-title">Organizer: <span class="foot-link"><a href="https://www.eee.hku.hk/" rel="external nofollow noopener" target="_blank">Faculty of Engineering, The University of Hong Kong</a></span></p> <p class="foot-title"> <span class="foot-link"><a href="https://www.eee.hku.hk/" rel="external nofollow noopener" target="_blank">Department of Electrical &amp; Electronic Engineering</a></span></p> <p class="foot-title">Coordinator: <span class="foot-link"><a href="https://www.eee.hku.hk/~evanpeng/" rel="external nofollow noopener" target="_blank">Evan Y. Peng, HKU EEE x CS</a></span></p> <br> <p class="foot-title "><b>Acknowledgement: </b></p> <div class="logos-container"> <a href="https://www.hku.hk" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/HKU.png" alt="Organization 1 Logo"> </a> <a href="https://www.stanford.edu/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/stanford.png" alt="Organization 2 Logo"> </a> <a href="https://www.kaist.ac.kr/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/KAIST.png" alt="Organization 3 Logo"> </a> <a href="https://www.pku.edu.cn/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/PKU.png" alt="Organization 4 Logo"> </a> <a href="https://www.zju.edu.cn/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/ZJU.png" alt="Organization 5 Logo"> </a> <a href="https://www.cityu.edu.hk/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/CityU.png" alt="Organization 6 Logo" class="special-logo-2"> </a> </div> </footer> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>