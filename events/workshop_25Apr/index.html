<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="crT26-QqFksmh-GiQYWgoAoNEhJdto58BzD3VJBNdWQ"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Events | WeLight</title> <meta name="author" content="Evan Y. Peng"> <meta name="description" content="Computational Imaging &amp; Mixed Representation Labortaty "> <meta name="keywords" content="Computational Imaging, Mixed Representation Labortaty"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?c4de7ed5661f0ca67e065788717516a0"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?c0069978495cdb6a4c56ca27eda22252"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hku.welight.fun/events/workshop_25Apr/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">WeLight</span> @ HKU</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item active"> <a class="nav-link" href="/events/">Events<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">PI</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <title>events</title> <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700;900&amp;display=swap" rel="stylesheet"> <script src="https://cdn.tailwindcss.com"></script> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css"> <style>.h1_1{font-size:40px!important;font-weight:900!important}.h2_1{color:#1f8f18!important;font-size:51px!important;font-weight:500!important;margin-top:-2.5rem!important;margin-bottom:4rem!important}.textarea{color:#FFF!important;font-weight:600!important;font-size:larger!important}.textareahead{color:#FFF!important;font-size:larger!important;font-size:30px!important;font-weight:700!important}.textarea{color:#FFF!important;font-weight:600!important;font-size:larger!important}.conference-background{display:flex;flex-direction:column;justify-content:center;align-items:start;background-image:url('/assets/img/bg_19.jpg');background-size:cover;background-attachment:fixed!important;background-position:center center;padding-top:5rem!important;padding-bottom:5rem!important;height:17rem;position:relative;margin-top:1.5rem;width:100vw;margin-left:-50vw;left:50%}.text-container{position:absolute;top:50%;left:calc(50% - min(40%,39rem));transform:translateY(-50%);width:auto}.header-container{display:flex;align-items:stretch;flex-wrap:wrap}.title-container{display:flex;flex-direction:column;justify-content:center;margin-bottom:-40px;max-width:917px!important}.image-container{position:relative;overflow:hidden;margin-left:auto;order:2;align-items:center;display:flex;padding-top:39px}.conference-image{max-width:100%;max-height:100px;object-fit:cover}@media(max-width:1536px){.image-container{margin-left:-5px;order:1;margin-bottom:-2rem;margin-top:-2rem}.title-container{order:2;width:100%}.conference-image{max-width:400px;max-height:120px;object-fit:cover}}body{font-family:'Poppins',sans-serif!important}.schedule{width:100%;margin:0;background:rgba(49,112,45,0.04);border-radius:0;overflow:hidden;box-shadow:rgba(49,112,45,0.4) 5px 5px,rgba(49,112,45,0.3) 10px 10px,rgba(49,112,45,0.2) 15px 15px,rgba(49,112,45,0.1) 20px 20px,rgba(49,112,45,0.05) 25px 25px;margin-bottom:4rem;padding:1rem}.session{display:flex;align-items:center;border-bottom:0 solid #eee;padding-left:1rem;padding-top:10px;padding-bottom:10px}.time{display:flex;justify-content:center;align-items:center;flex:0 0 20%;font-weight:600;color:#1f8f18;margin-right:1rem}.details{flex:1}.title{font-size:18px;margin-bottom:5px;font-weight:600}.speaker{font-size:16px;color:#666}.speaker-container{display:flex;flex-wrap:wrap}.speaker-profile{align-items:flex-start;gap:1rem;margin-bottom:2rem}.speaker-photo{width:12rem;object-fit:cover;min-height:10rem;max-height:13rem;margin-right:2rem;margin-top:7px;margin-bottom:1.5rem;float:left}.speaker-content{flex:1}.speaker-name{font-weight:bold;font-size:larger}.speaker-title{font-size:16px;color:#666}.speaker-link a{color:#1f8f18;text-decoration:none}.speaker-abstract{font-style:normal}.speaker-bio{font-weight:normal}.conference-materials{margin:2rem auto}.material-detail{background-color:rgba(49,112,45,0.04);margin-bottom:1rem;transition:all .3s ease-in-out}.material-summary{font-weight:600;color:#1f8f18;padding:1rem;margin:0;cursor:pointer;outline:0;position:relative;list-style:none;transition:all .3s ease-in-out}.material-summary::after{content:'▼';position:absolute;right:1rem;transition:transform .3s ease-in-out}.material-detail[open] .material-summary::after{transform:rotate(180deg)}.material-content{padding:1rem;line-height:1.6;max-height:0;overflow:overlay;transition:max-height .3s ease-in-out}.material-content b{font-weight:600}s .material-content ol{padding-left:10px;list-style-type:none;margin-top:1rem}.material-content a{color:#1f8f18;text-decoration:none}.material-content ol li{position:relative;padding-left:20px;margin-bottom:10px;margin-top:1rem}.material-content ol li::before{content:"•";color:#1f8f18;font-weight:bold;position:absolute;left:0;top:0;font-size:20px}.material-content-name{font-weight:600;font-size:initial;margin-top:-1rem;margin-bottom:1rem}.material-detail[open] .material-content{max-height:30rem}.material-summary:hover{background-color:var(--global-hover-colorr)}details>summary{list-style:none}details>summary::-webkit-details-marker{display:none}.logos-container{display:flex;justify-content:center;align-items:center;gap:20px;margin-top:1rem;flex-wrap:wrap}.logos-container img{max-height:min(7rem,18vw);width:auto;transition:transform .3s ease}.logos-container img:hover{transform:scale(1.1)}.special-logo-1{max-width:max(5rem,7vw)}.special-logo-2{max-width:max(6rem,8vw)}.foot-title{font-size:24px;color:#666;font-weight:700}.foot-link a{color:#1f8f18;text-decoration:none;font-size:24px;font-weight:300}.foot-title b{font-size:32px;font-weight:700}.register-button{display:inline-block;padding:10px 20px;text-align:center;color:white;text-decoration:none!important;font-size:21px;position:relative;overflow:hidden;margin-bottom:1rem;font-weight:600;border-radius:5px;background:#557750;transition:background-color .3s,transform .3s,box-shadow .3s,padding-right .3s;box-shadow:0 4px 10px rgba(0,0,0,0.20)}.register-button::after{content:'→';position:absolute;right:-20px;top:50%;transform:translateY(-50%);transition:right .3s ease}
.register-button:hover{padding-right:40px;background-color:#557750;color:white!important;text-decoration:none;box-shadow:0 12px 24px rgba(0,0,0,0.20);transform:translateY(-3px)}.register-button:hover::after{right:10px}.modal{display:none;position:fixed;z-index:1;left:0;top:0;width:100%;height:100%;overflow:auto;background-color:rgba(0,0,0,0.4)}.modal-content{background-color:#fefefe;position:absolute;top:50%;left:50%;transform:translate(-50%,-50%);padding:20px;border:1px solid #888;width:90%;max-width:1200px;height:85%;overflow:auto;box-shadow:0 5px 15px rgba(0,0,0,0.3);border-radius:8px}.close{color:#aaa;float:right;font-size:28px;font-weight:bold}.close:hover,.close:focus{color:black;text-decoration:none;cursor:pointer}.nav-link{display:inline-block;font-size:16px;font-weight:bold;color:#fff;text-decoration:none;background-color:rgba(255,255,255,0.3);padding:8px 12px;border-radius:5px;backdrop-filter:blur(10px);box-shadow:0 2px 4px rgba(0,0,0,0.1);transition:background-color .3s,color .3s}.nav-link:hover{background-color:rgba(0,0,0,0.5);color:#fff}</style> <div class="container mx-auto px-4"> <div class="header-container"> <div class="title-container"> <h1 class="h1_1 text-6xl font-bold mt-10 mb-5 "> Workshop on</h1> <h2 class="h2_1 text-5xl font-bold mb-5">Frontiers of Image Science and Visual Computing 2025</h2> </div> <div class="image-container"> <img src="/assets/img/logos/University_of_Hong_Kong_Logo.png" alt="HKU" class="conference-image"> </div> </div> <div class="bg-gray-200 p-5 mb-5 conference-background"> <div class="text-container"> <h3 class="text-3xl font-bold mb-2 textareahead">Let there be light</h3> <p class="mb-2 textarea">15-16 April, 2025</p> <p class="mb-2 textarea">3/F, Main Library, The University of Hong Kong, HONG KONG SAR</p> <a href="#" class="mb-2 nav-link" onclick="openModal()">Navigate Here</a> </div> </div> <div id="modal" class="modal"> <div class="modal-content"> <span class="close" onclick="closeModal()">×</span> <iframe id="modal-iframe" src="/assets/pdf/Navigation.pdf" width="100%" height="100%"></iframe> </div> </div> <h2 class="text-4xl font-bold mb-5">Workshop Objectives</h2> <div class="grid grid-cols-1 md:grid-cols-2 mb-10"> <div> <div class="flex items-start mb-2" style="margin-bottom: 2rem !important"> <div class="text-red-500 mr-2"> <i class="fas fa-circle"></i> </div> <div> <h4 class="font-bold">Encourage Innovative Spirit</h4> </div> </div> <div class="flex items-start mb-2" style="margin-bottom: 2rem !important"> <div class="text-red-500 mr-2"> <i class="fas fa-circle"></i> </div> <div> <h4 class="font-bold ">Promote Excellence and Sustain Quality</h4> </div> </div> </div> <div> <div class="flex items-start mb-2" style="margin-bottom: 2rem !important"> <div class="text-red-500 mr-2"> <i class="fas fa-circle"></i> </div> <div> <h4 class="font-bold ">Strive for Improvement</h4> </div> </div> <div class="flex items-start" style="margin-bottom: 2rem !important"> <div class="text-red-500 mr-2"> <i class="fas fa-circle"></i> </div> <div> <h4 class="font-bold ">Connect Communities</h4> </div> </div> </div> </div> <h2 class="text-4xl font-bold mb-5" style="margin-top: -2rem !important">Schedule</h2> <a href="https://example.com" target="_blank" class="register-button" rel="external nofollow noopener">Register Now (Free)</a> <div class="schedule" id="schedule"></div> <script>function openModal(){document.getElementById("modal").style.display="flex"}function closeModal(){document.getElementById("modal").style.display="none"}const sessions=[{title:"Tuesday, 15 April",speaker:'MC: <a href="https://www.eee.hku.hk/~evanpeng/" target="_blank">Evan Y. Peng, HKU</a>',showTime:!1},{title:"Registration",duration:30,showTime:!0},{title:"Opening Remarks",duration:10,speaker:"TBD",showTime:!0},{title:'The Joy of Latents: What Image Generators "Know" and Don\'t "Know", and Why It Matters',duration:50,speaker:"David Forsyth, UIUC",showTime:!0},{title:"Diffusion Posterior Sampling for Computational Imaging with Inaccurate Priors or Physics",duration:25,speaker:"He Sun, PKU",showTime:!0},{title:"Learning 3D Representations from Videos",duration:25,speaker:"Xiaojuan Qi, HKU",showTime:!0},{title:"Group Photo",duration:10,showTime:!0},{title:"Lunch",duration:90,showTime:!0},{title:"Security Concerns on Visual AI beyond RGB Domain",duration:30,speaker:"Yinqiang Zheng, UTokyo",showTime:!0},{title:"Differentiable Acquisition of Appearance and Geometry",duration:25,speaker:"Hongzhi Wu, ZJU",showTime:!0},{title:"Computational Photography in the Age of Foundation and Generative Models",duration:25,speaker:"Tianfan Xue, CUHK",showTime:!0},{title:"Coffee & Tea Break",duration:20,showTime:!0},{title:"Flat-Optics for Imaging with Extended Information",duration:25,speaker:"Yuanmu Yang, THU",showTime:!0},{title:"In Quest of Better Gaussian Splatting",duration:25,speaker:"Ping Tan, HKUST",showTime:!0},{title:"Computational Imaging for Graphics, Vision, Robotics, and Science",duration:25,speaker:"Seung-Hwan Baek, POSTECH",showTime:!0},{title:"ACM SIGGRAPH Asia 2025 Pitching",duration:5,speaker:"Taku Komura, HKU",showTime:!0},{title:"Closing Remarks & Networking",duration:30,speaker:"TBD",showTime:!0},{title:"Dinner Banquet and HK-Night Tour",Time:"18:30",duration:150,speaker:"invitation only",showTime:!0},{title:"Wednesday, 16 April",showTime:!1},{title:"Fireside Chat (Research) and Lab Tour",Time:"10:00",duration:90,speaker:"invitation only",showTime:!0},{title:"Lunch-on Discussion and Sharing",duration:60,speaker:"invitation only",showTime:!0}];let currentTime=new Date;currentTime.setHours(10,0);const scheduleContainer=document.getElementById("schedule");scheduleContainer?sessions.forEach(e=>{const t=document.createElement("div");t.className="session";let i="";if(e.hasOwnProperty("Data"))i=e.Data;else if(e.hasOwnProperty("Time")&&e.hasOwnProperty("duration")){const[t,n]=e.Time.split(":").map(Number);currentTime.setHours(t,n);const o=new Date(currentTime.getTime()+6e4*e.duration);i=`${currentTime.toTimeString().substring(0,5)} - ${o.toTimeString().substring(0,5)}`,currentTime=o}else if(e.hasOwnProperty("duration")){const t=new Date(currentTime.getTime()+6e4*e.duration);i=`${currentTime.toTimeString().substring(0,5)} - ${t.toTimeString().substring(0,5)}`,currentTime=t}t.innerHTML=`\n          <div class="time">${e.showTime?i:""}</div>\n          <div class="details">\n            <div class="title" ${e.showTime?"":'style="color: #1f8f18 !important"'}>${e.title}</div>\n            ${e.speaker?`<div class="speaker">${e.speaker}</div>`:""}\n          </div>\n        `,scheduleContainer.appendChild(t)}):console.error("Schedule container not found"),window.onclick=function(e){const t=document.getElementById("modal");e.target==t&&closeModal()};</script> <h2 class="text-4xl font-bold mb-5">Speakers/Guests</h2> <div class="speaker-container"> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/David%20Forsyth.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">David Forsyth</h2> <h3 class="speaker-title">ACM Fellow, IEEE Fellow, University of Illinois Urbana-Champaign (UIUC)</h3> <p class="speaker-link"><a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> David Alexander Forsyth is currently Fulton-Watson-Copp chair in computer science at U. Illinois at Urbana-Champaign, where he moved from U.C Berkeley, where he was also full professor. Prof. Forsyth has occupied the Fulton-Watson-Copp chair in Computer Science at the University of Illinois since 2014. He has published over 170 papers on computer vision, computer graphics and machine learning. He has served as program co-chair for IEEE Computer Vision and Pattern Recognition in 2000, 2011, 2018 and 2021, general co-chair for CVPR 2006 and 2015 and ICCV 2019, program co-chair for the European Conference on Computer Vision 2008, and is a regular member of the program committee of all major international conferences on computer vision. He has served six years on the SIGGRAPH program committee, and is a regular reviewer for that conference. He has received best paper awards at the International Conference on Computer Vision and at the European Conference on Computer Vision. He received an IEEE technical achievement award for 2005 for his research. Prof. Forsyth became an IEEE Fellow in 2009, and an ACM Fellow in 2014. His textbook, "Computer Vision: A Modern Approach" (joint with J. Ponce and published by Prentice Hall) is now widely adopted as a course text (adoptions include MIT, U. Wisconsin-Madison, UIUC, Georgia Tech and U.C. Berkeley). A further textbook, “Probability and Statistics for Computer Science”, is in print; yet another (“Applied Machine Learning”) has just appeared. He has served two terms as Editor in Chief, IEEE TPAMI. He has served on a number of scientific advisory boards. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Yinqiang%20Zheng.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Yinqiang Zheng</h2> <h3 class="speaker-title">The University of Tokyo (UTokyo)</h3> <p class="speaker-link"><a href="https://researchmap.jp/yinqiangzheng?lang=en" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Dr. Yinqiang Zheng received his Doctoral degree of engineering from the Department of Mechanical and Control Engineering, Tokyo Institute of Technology, Tokyo, Japan, in 2013. He is currently a Full Professor in the Next Generation Artificial Intelligence Research Center, The University of Tokyo, Japan, leading the Optical Sensing and Camera System Laboratory (OSCARS Lab.). He has published a serial of research papers that bridge AI and optical imaging, surrounding the novel paradigms of ‘Optics for Better AI’ and ‘AI for Best Optics’. He has served as area chair for CVPR, ICCV, ICML, ICLR, NeurIPS, MM, 3DV, ACCV, ISAIR, DICTA and MVA. He is a foreign fellow of the Engineering Academy of Japan, and the recipient of the Konica Minolta Image Science Award and Funai Academic Award. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Seung-Hwan%20Baek.png" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Seung-Hwan Baek </h2> <h3 class="speaker-title"> Pohang University of Science and Technology (POSTECH)</h3> <p class="speaker-link"><a href="https://www.shbaek.com/home" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Seung-Hwan Baek is an Assistant Professor of Computer Science and Engineering at POSTECH, also affiliated with the Graduate School of AI, POSTECH. He leads POSTECH Computational Imaging Group and serves as a faculty co-director of POSTECH Computer Graphics Lab. Prof. Baek worked as a postdoctoral researcher at Princeton University, and obtained his Ph.D. in Computer Science from KAIST. Prof. Baek's research, situated at the intersection of computer graphics, vision, AI, and optics, focuses on capturing, modeling, and analyzing high-dimensional visual data originating from complex interplays between light, material appearance, and geometry. Prof. Baek has received awards, including Frontiers of Science Award from the International Congress of Basic Science, Outstanding Ph.D. Thesis Award in IT from the Korean Academy of Science and Technology, SIGGRAPH Asia Doctoral Consortium, Microsoft Research Asia Ph.D. Fellowship, and both ACCV Best Application Paper Award and Best Demo Award. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Yuanmu%20Yang.png" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Yuanmu Yang</h2> <h3 class="speaker-title">Tsinghua University (THU)</h3> <p class="speaker-link"><a href="https://ymyanggroup.com/people/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Yuanmu Yang is an Associate Professor in the Department of Precision Instrument at Tsinghua University. He earned his B.Eng. in Optoelectronics from Tianjin University and Nankai University in 2011 and his Ph.D. in Interdisciplinary Materials Science from Vanderbilt University in 2015. He was a postdoctoral researcher at Sandia National Laboratories, USA from 2015 to 2017. He then worked at Intellectual Ventures, a Seattle-based startup incubator, from 2017 to 2018, and was a founding team member of metasurface-based solid-state lidar startup Lumotive. His research focuses on the area of meta-optics. He has published more than 50 journal articles, including 2 in Nature Photonics and 1 in Nature Physics, received over 6000 citations according to Google Scholar, and was selected as a “Highly-cited Researcher in China” by Elsevier in 2023. He has been granted over 10 China and US patents. His recognitions include the Jin-Guofan Young Scientist Award given by the China Instrument and Control Society as well as the Forbes China “30 under 30” in 2018. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/He%20Sun.png" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">He Sun</h2> <h3 class="speaker-title">Peking University (PKU)</h3> <p class="speaker-link"><a href="https://ai4scientificimaging.org/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Dr. He Sun is an Assistant Professor in the National Biomedical Imaging Center, Peking University, China. Prior to starting at Peking University, he was a postdoctoral researcher in the Department of Computing and Mathematical Sciences at California Institute of Technology. He received his Ph.D. in Mechanical and Aerospace Engineering from Princeton University in 2019 and his bachelor's degree in Engineering Mechanics and Economics from Peking University in 2014. His research primarily focuses on computational imaging, which tightly integrates optics, control, signal processing and machine learning to push the boundary of scientific imaging. His past work has contributed to multiple real science missions, including the Event Horizon Telescope for black hole interferometric imaging. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Hongzhi%20Wu.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Hongzhi Wu</h2> <h3 class="speaker-title">Zhejiang University (ZJU)</h3> <p class="speaker-link"><a href="https://svbrdf.github.io/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Hongzhi Wu is a Full Professor in the State Key Lab of CAD&amp;CG, Zhejiang University, China. He obtained his Ph.D. from Yale University, under the supervision of Julie Dorsey and Holly Rushmeier, and was awarded Excellent Young Scholar by NSF China. His research focuses on the acquisition and reconstruction of physical information, including complex high-dimensional appearance, 3D surfaces and volumes. Hongzhi's research leads to publications in top venues including SIGGRAPH/SIGGRAPH Asia, CVPR and ICCV, two published books, as well as a number of state-of-the-art high-performance illumination multiplexing devices. He was the recipient of Luzengyong CAD&amp;CG High Tech Award (1st prize). Hongzhi is on the editorial board of IEEE TVCG, JCAD &amp; VCIBA, and has served on the program committees of various international conferences, including SIGGRAPH, SIGGRAPH Asia, VR, EG, PG, EGSR, I3D and HPG. More details can be found from his homepage https://svbrdf.github.io/. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Jingyi%20Yu.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Jingyi Yu</h2> <h3 class="speaker-title">Optica Fellow, IEEE Fellow, ShanghaiTech University (ShanghaiTech)</h3> <p class="speaker-link"><a href="https://www.yu-jingyi.com/index.html#" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Prof. Jingyi Yu is the Vice Provost of ShanghaiTech University and Dean of its School of Information Science and Technology. He earned his B.S. from Caltech in 2000 and his Ph.D. from MIT in 2005. Before joining ShanghaiTech, he was a full professor at the University of Delaware. His research in computer vision and graphics—especially computational photography and innovative optics—has produced over 120 publications (70+ in top venues such as CVPR, ICCV, ECCV, and TPAMI) and 10 US patents. His work has been supported by agencies like NSF, NIH, ARO, and AFOSR, and he has received awards including the NSF CAREER Award, the AFOSR YIP Award, and the Outstanding Junior Faculty Award. He has also held leadership roles at major international conferences, serves as an associate editor for IEEE TPAMI, IEEE TIP, and Elsevier CVIU, and is the program chair for ICPR 2020, CVPR 2021, WACV 2021, and ICCV 2025. Additionally, he has been elevated to IEEE Fellow and Optica Fellow for his contributions to the field. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Ping%20Tan.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Ping Tan</h2> <h3 class="speaker-title">Hong Kong University of Science and Technology (HKUST)</h3> <p class="speaker-link"><a href="https://pingtan.people.ust.hk/index.html" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Dr. Tan is a Professor in the Department of Electronic and Computer Engineering at the Hong Kong University of Science and Technology (HKUST). Before joining HKUST, he served as the director of the XR Lab at Alibaba DAMO Academy from 2019 to 2022, an Associate Professor at Simon Fraser University (SFU) in Canada from 2014 to 2019, and an assistant and Associate Professor at the National University of Singapore (NUS) from 2007 to 2014. Dr. Tan received his PhD from HKUST in 2007 and his Master's and Bachelor's degrees from Shanghai Jiao Tong University (SJTU) in 2003 and 2000, respectively. He specializes in computer vision, computer graphics, and robotics, with a research focus on 3D vision. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Tianfan%20Xue.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Tianfan Xue</h2> <h3 class="speaker-title">The Chinese University of Hong Kong (CUHK)</h3> <p class="speaker-link"><a href="https://tianfan.info/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Professor Tianfan Xue is an Assistant Professor in the Department of Information Engineering at the Chinese University of Hong Kong. Prior to this, he spent over five years as a Senior Staff Engineer at Google Research. He earned his Ph.D. from the Computer Science and Artificial Intelligence Laboratory (CSAIL) at the Massachusetts Institute of Technology (MIT) under the supervision of Professor William T. Freeman. In 2011, he obtained his M.Phil. from the Chinese University of Hong Kong under the guidance of Professor Xiaoou Tang, and in 2009, he received his bachelor's degree from Tsinghua University. His research focuses on computational photography, computer vision and graphics, and machine learning. His work on reflectance technology has been adopted by Google Photoscan, an app with over 10 million users; his fast bilateral learning technique has been integrated into Google Tensor chips; his night photography algorithm won DPReview's Best Innovation Award; and his bilateral NeRF algorithm received a Best Paper honorable mention at SIGGRAPH. He also serves as a reviewer for various top conferences and journals and has been a web chair or area chair for conferences such as CVPR, ICCV, WACV, ACM MM, and NeurIPS. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Xiaojuan%20Qi.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Xiaojuan Qi</h2> <h3 class="speaker-title">The University of Hong Kong (HKU)</h3> <p class="speaker-link"><a href="https://xjqi.github.io/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Xiaojuan Qi is currently an Assistant Professor in the Department of Electrical and Electronic Engineering, the University of Hong Kong (HKU). Before joining HKU, she was a postdoctoral researcher at the University of Oxford, UK. She received her Ph.D. from the Chinese University of Hong Kong (CUHK) in 2018 and her B.Eng. from Shanghai Jiao Tong University (SJTU) in 2014. From September 2016 to November 2016, she was a visiting student in the Machine Learning Group, University of Toronto. She has carried out an internship at Intel Intelligent Systems Lab from May 2017 to November 2017. She has won several awards such as the first place of ImageNet Semantic Parsing Challenge, Outstanding Reviewer in ICCV’17 and ICCV’19, CVPR’18 Doctoral Consortium Travel Award and Hong Kong PhD Fellowship (2014 – 2018). </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Taku%20Komura.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Taku Komura</h2> <h3 class="speaker-title">General Chair of ACM SIGGRAPH Asia 2025, The University of Hong Kong (HKU)</h3> <p class="speaker-link"><a href="https://i.cs.hku.hk/~taku/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Taku Komura joined the University of Hong Kong in 2020. Before joining HKU, he worked at the University of Edinburgh (2006-2020), City University of Hong Kong (2002-2006) and RIKEN (2000-2002). He received his BSc, MSc and PhD in Information Science from University of Tokyo. His research has focused on data-driven character animation, physically-based character animation, crowd simulation, 3D modelling, cloth animation, anatomy-based modelling and robotics. Recently, his main research interests have been on physically-based animation and the application of machine learning techniques for animation synthesis. He received the Royal Society Industry Fellowship (2014) and the Google AR/VR Research Award (2017). </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/hongbo_2019.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Hongbo Fu</h2> <h3 class="speaker-title">Technical Papers Assistant Chair of SIGGRAPH Asia 2025, HKUST</h3> <p class="speaker-link"><a href="https://hongbofu.people.ust.hk/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Hongbo Fu is a Professor at the Division of Emerging Interdisciplinary Areas, HKUST. He also serve as the Technical Papers Assistant Chair for SIGGRAPH Asia 2025. Prior to his current appointment, he worked at the School of Creative Media, City University of Hong Kong, for over 15 years. He had postdoctoral research training at the Imager Lab, University of British Columbia, Canada, and the Department of Computer Graphics, Max-Planck-Institut Informatik, Germany. He received a Ph.D. degree in computer science from HKUST in 2007 and a BS degree in information sciences from Peking University, China, in 2002. His primary research interests fall in computer graphics, human-computer interaction, and computer vision. His research has led to over 100 scientific publications, including 70+ papers in the best graphics/vision journals and 30+ papers in the best vision/HCI conferences. His recent works have received a Silver Medal from Special Edition 2022 Inventions Geneva Evaluation Days, the Best Demo awards at the Emerging Technologies program, ACM SIGGRAPH Asia in 2013 and 2014, and the Best Paper awards from CAD/Graphics 2015 and UIST 2019. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Wenzheng.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Wenzheng Chen</h2> <h3 class="speaker-title">Peking University (PKU)</h3> <p class="speaker-link"><a href="https://wenzhengchen.github.io/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Wenzheng Chen is a tenure-track Assistant Professor at Wangxuan Institute of Computer Technology, Peking University, where he specializes in computational photography and 3D vision. He is also affiliated with the Visual Computing and Learning Lab. Prior to joining Peking University, he served as a research scientist at NVIDIA Toronto AI Lab. He received his Ph.D. from the University of Toronto and earned both his Master's and Bachelor's degrees from Shandong University. His research primarily explores the integration of various imaging systems—including digital cameras, LiDAR, structured light, and SPAD—with deep learning frameworks to enhance 3D perception by accurately predicting attributes such as geometry, texture, surface material, and environmental lighting. His innovative work in differentiable rendering has successfully transformed into an Omniverse product, while his structured light research earned the ICCP 2021 Best Poster Award and his work in non-line-of-sight imaging has been exhibited at the Princeton Art of Science Exhibition. </p> </div> </div> </div> <div class="conference-materials"> <h2 class="text-4xl font-bold mb-5">Workshop Materials</h2> <details class="material-detail"> <summary class="material-summary">The Joy of Latents: What Image Generators "Know" and Don't "Know", and Why It Matters <span style="color:black;"> - David Forsyth</span></summary> <div class="material-content"> <p> I will discuss three mysteries at the heart of the current vision agenda: What do image generators "know" and not "know"? What should be represented explicitly? How can we tell how well an image generator works? <br><br> I will show strong evidence that depth, normal and albedo can be extracted from two kinds of image generator, with minimal inconvenience or training data. This suggests that image generators might "know" more about scene appearance than we realize. I will show that there are important scene properties that image generators very reliably get wrong. These include shadow geometry and perspective geometry. Similarly, video generators get object constancy and properties like momentum conservation wrong. <br><br> If generators know intrinsics, why extract them from images? Perhaps because some other downstream process -- rendering, movement, whatever -- will use them. But rendering seems to work much better if you use latents, and numerous systems move using entirely latent representations. The only good argument in favor of an exposed image representation is that it might be easy to interact or compute with. This suggests paying more attention to representations that suppress detail in favor of capturing major effects in a compact form. I will show a method capable of computing such representations very efficiently for vast numbers of images. <br><br> Fixing image generators requires an evaluation procedure that will tell you if the generator got better. Image generators are evaluated using entirely irrational procedures (doesn't this look good; FID; and so on). But, looked at closely, most image generators and conditional image generators don't work very well. Part of the problem is we simply don't know how to do evaluate them in a sensible way. I will discuss some progress in this respect. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Diffusion Posterior Sampling for Computational Imaging with Inaccurate Priors or Physics <span style="color:black;"> - He Sun</span> </summary> <div class="material-content"> <p> Diffusion models excel in solving imaging inverse problems due to their exceptional ability to model complex image priors. When integrated with image formation models, they enable a physics-guided diffusion process—termed diffusion posterior sampling (DPS)—that effectively retrieves high-dimensional posterior distributions of images from corrupted observations. However, DPS’s dependence on clean diffusion priors and accurate imaging physics limits its practical utility, especially when clean data is scarce for pre-training diffusion models or when the underlying physical models are inaccurate for inference. However, DPS requires clean diffusion priors and accurate imaging physics for inference, limiting its practical use when clean data is scarce for pre-training diffusion models or when the underlying physical models are inaccurate or unknown. In this talk, Prof. Sun will introduce an expectation-maximization (EM) framework that adapts DPS for scenarios with inaccurate priors or physics. This method alternates between an E-step, in which clean images are reconstructed from corrupted data assuming known diffusion prior and physical model, and an M-step, where these reconstructions are employed to refine and recalibrate the prior or physical model. This iterative process progressively aligns the learned image prior with the true clean distribution and adapts the physical model for enhanced accuracy. They validate their approach through extensive experiments across diverse computational imaging tasks—including inpainting, deblurring, denoising, and realistic microscopic imaging—demonstrating new state-of-the-art performance. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Learning 3D Representations from Videos <span style="color:black;"> - Xiaojuan Qi</span></summary> <div class="material-content"> <p> Humans navigate a rich 3D world, continuously acquiring diverse skills and engaging in various activities through perception, understanding, and interaction. The long-term research goal of Prof. Qi's group is to simulate the dynamic 3D world and equip AI systems with 3D spatial understanding. A fundamental challenge in achieving this lies in how to effectively represent 3D structures. In this talk, Prof. Qi will present a series of research efforts from her group focused on learning 3D representations from videos for surface reconstruction, novel view synthesis, and 4D modeling of dynamic scenes, with applications in video processing. She will also discuss future directions toward generating editable 4D worlds from casually captured videos. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Security Concerns on Visual AI beyond RGB Domain <span style="color:black;"> - Yinqiang Zheng</span></summary> <div class="material-content"> <p> AI algorithms for computer-based visual understanding have advanced significantly, due to the prevalence of deep learning and large-scale visual datasets in the RGB domain, which have also been proven vulnerable to digital and physical adversarial attacks. To deal with complex scenarios, many other imaging modalities beyond the visibility scope of human eyes, such as near infrared (NIR), short wavelength infrared (SWIR), thermal infrared (TIR), polarization, neuromorphic pulse, have been introduced, yet the vulnerabilities of visual AI based on these non-RGB modalities have not received due attention. In this talk, Prof. Zheng will show that typical AI algorithms, like object detection and segmentation, can be more fragile than in the RGB domain, and the properly crafted attackers can hardly be noticed by naked human eyes. They showcase a serial of physics-based attackers in the NIR, SWIR, and TIR domain, a printable attacker for event-based human detection, and a projection-based attacker on polarization-based reconstruction and segmentation. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Differentiable Acquisition of Appearance and Geometry <span style="color:black;"> - Hongzhi Wu</span></summary> <div class="material-content"> <p> High-quality, efficient acquisition of complex appearance and geometry is a classic problem in computer graphics and vision. Differentiable acquisition maps both physical acquisition and computational reconstruction to a differentiable pipeline, enabling a fully automatic, joint optimization of hardware and software. This significantly improves modeling efficiency and quality over existing work. In this talk, Prof. Wu will briefly introduce his recent research on differential acquisition, including OpenSVBRDF, the first large-scale measured SVBRDF database (SIGAsia 2023a), the first dynamic visible-light tomography system (SIGAsia 2023b), as well as a system for real-time acquisition and reconstruction of dynamic 3D volumes (CVPR 2024). He will also describe on-going research on building a dedicated high-performance device for acquiring the appearance of cultural relics in the National Museum of China. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Computational Photography in the Age of Foundation and Generative Models<span style="color:black;"> - Tianfan Xue</span></summary> <div class="material-content"> <p> Computational photography is widely used in our daily lives, from the convenience of smartphone photography, image enhancement, and short video filters to more advanced applications like autonomous driving, medical imaging, and scientific observation. With the rapid development of generative models and neural rendering fields, image generation has made significant strides, raising a challenging question: can traditional computational photography be fully replaced by large models? <br><br> In this talk, we will explore how to combine traditional computational photography techniques with foundational and generative models. Specifically, we will address the following four aspects: first, how to use generative models to enhance existing image processing, such as UltraFusion HDR, where we will demonstrate achieving high dynamic range fusion of up to 9 stops for the first time using generative models; second, how to leverage generative model priors to improve the imaging quality of novel camera systems (e.g., lensless cameras and event cameras); third, how foundational models can automate camera system development, including automated evaluation (DepictQA) and automated debugging (RLPixTuner); and finally, the application of large models in 3D reconstruction and video frame interpolation. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Flat-Optics for Imaging with Extended Information <span style="color:black;"> - Yuanmu Yang</span></summary> <div class="material-content"> <p> Conventional cameras can only acquire light intensity in two dimensions. In order to further obtain the depth, polarization, and spectral information of the target object or to achieve imaging resolution beyond the diffraction limit, it is often required to use bulky and expensive instruments. Here, Prof. Yang will present his group’s recent effort to replace conventional camera lenses with flat diffractive optical elements or metasurfaces. By leveraging the unique capability of metasurface to tailor the vectorial field of light, in combination with an advanced image retrieval algorithm, they aim to build compact camera systems that can capture multi-dimensional light field information of a target scene in a single shot under ambient illumination conditions. Specifically, he will show how they use flat-optics to build a monocular camera that can capture a 4D image, including 2D all-in-focus intensity, depth, and polarization of a target scene in a single shot over extended scenes. He would also like to present their effort to construct flat-optics-based monocular 3D camera modules for real-world applications. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">In Quest of Better Gaussian Splatting <span style="color:black;"> - Ping Tan</span></summary> <div class="material-content"> <p> Coming soon. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Computational Imaging for Graphics, Vision, Robotics, and Science <span style="color:black;"> - Seung-Hwan Baek</span></summary> <div class="material-content"> <p> In this talk, Prof. Baek will discuss his recent work on designing, controlling, and utilizing computational imaging systems for inverse rendering, robotic vision in extreme conditions, transparent and metameric material analysis, and advanced VR/AR systems. The core idea is to take a holistic approach—considering rendering, the transformation of multi-dimensional light properties as they travel from the source through a scene, their interaction with imaging systems, and the computational processes for reconstruction and restoration. </p> </div> </details> </div> <footer style="text-align:center; padding:20px;"> <p class="foot-title">Organizer: <span class="foot-link"><a href="https://hku.welight.fun/">Computational Imaging &amp; Mixed Representation Laboratory</a></span></p> <p class="foot-title">Local Coordinating Team: <span class="foot-link"><a href="https://andrewbxy.github.io/" rel="external nofollow noopener" target="_blank">Xiaoyang Bai, </a><a href="https://liux2018.github.io/" rel="external nofollow noopener" target="_blank">Xin Liu, </a><a href="https://qufeifan.github.io/" rel="external nofollow noopener" target="_blank">Feifan Qu, </a><a href="https://liangxunou.github.io/" rel="external nofollow noopener" target="_blank">Liangxun Ou</a></span></p> <p class="foot-title">Advisory Committee: <span class="foot-link"><a href="https://vccimaging.org/People/heidriw/" rel="external nofollow noopener" target="_blank">Wolfgang Heidrich, HKU/KAUST &amp; </a><a href="https://www.eee.hku.hk/~huangkb/" rel="external nofollow noopener" target="_blank">Kaibin Huang, HKU</a></span></p> <p class="foot-title">Chair: <span class="foot-link"><a href="https://www.eee.hku.hk/~evanpeng/" rel="external nofollow noopener" target="_blank">Evan Y. Peng, HKU EEE x CS</a></span></p> <br> <p class="foot-title "><b>Acknowledgement: </b></p> <div class="logos-container"> <a href="https://www.hku.hk" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/HKU.png" alt="Organization Logo"> </a> <a href="https://illinois.edu/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/UIUC.png" alt="Organization Logo"> </a> <a href="https://www.postech.ac.kr/eng/index.do" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/POSTECH.png" alt="Organization Logo"> </a> <a href="https://www.u-tokyo.ac.jp/en/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/UTokyo.png" alt="Organization Logo"> </a> <a href="https://www.tsinghua.edu.cn/en/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/THU.png" alt="Organization Logo"> </a> <a href="https://english.pku.edu.cn/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/PKU.png" alt="Organization Logo"> </a> <a href="https://www.zju.edu.cn/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/ZJU.png" alt="Organization Logo"> </a> <a href="https://www.shanghaitech.edu.cn/eng/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/ShanghaiTech.png" alt="Organization Logo"> </a> <a href="https://hkust.edu.hk/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/HKUST.png" alt="Organization Logo"> </a> <a href="https://www.cuhk.edu.hk/english/index.html" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/CUHK.png" alt="Organization Logo" class="special-logo-2"> </a> </div> </footer> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>