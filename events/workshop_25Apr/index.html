<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="crT26-QqFksmh-GiQYWgoAoNEhJdto58BzD3VJBNdWQ"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Events | WeLight</title> <meta name="author" content="Evan Y. Peng"> <meta name="description" content="Computational Imaging &amp; Mixed Representation Labortaty "> <meta name="keywords" content="Computational Imaging, Mixed Representation Labortaty"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?c4de7ed5661f0ca67e065788717516a0"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?c0069978495cdb6a4c56ca27eda22252"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hku.welight.fun/events/workshop_25Apr/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">WeLight</span> @ HKU</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item active"> <a class="nav-link" href="/events/">Events<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">PI</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <title>events</title> <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700;900&amp;display=swap" rel="stylesheet"> <script src="https://cdn.tailwindcss.com"></script> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css"> <style>.h1_1{font-size:40px!important;font-weight:900!important}.h2_1{color:#1f8f18!important;font-size:51px!important;font-weight:500!important;margin-top:-2.5rem!important;margin-bottom:4rem!important}.textarea{color:#FFF!important;font-weight:600!important;font-size:larger!important}.textareahead{color:#FFF!important;font-size:larger!important;font-size:30px!important;font-weight:700!important}.textarea{color:#FFF!important;font-weight:600!important;font-size:larger!important}.conference-background{display:flex;flex-direction:column;justify-content:center;align-items:start;background-image:url('/assets/img/bg_19.jpg');background-size:cover;background-attachment:fixed!important;background-position:center center;padding-top:5rem!important;padding-bottom:5rem!important;height:17rem;position:relative;margin-top:1.5rem;width:100vw;margin-left:-50vw;left:50%}.text-container{position:absolute;top:50%;left:calc(50% - min(40%,39rem));transform:translateY(-50%);width:auto}.header-container{display:flex;align-items:stretch;flex-wrap:wrap}.title-container{display:flex;flex-direction:column;justify-content:center;margin-bottom:-40px;max-width:917px!important}.image-container{position:relative;overflow:hidden;margin-left:auto;order:2;align-items:center;display:flex;padding-top:39px}.conference-image{max-width:100%;max-height:100px;object-fit:cover}@media(max-width:1536px){.image-container{margin-left:-5px;order:1;margin-bottom:-2rem;margin-top:-2rem}.title-container{order:2;width:100%}.conference-image{max-width:400px;max-height:120px;object-fit:cover}}body{font-family:'Poppins',sans-serif!important}.schedule{width:100%;margin:0;background:rgba(49,112,45,0.04);border-radius:0;overflow:hidden;box-shadow:rgba(49,112,45,0.4) 5px 5px,rgba(49,112,45,0.3) 10px 10px,rgba(49,112,45,0.2) 15px 15px,rgba(49,112,45,0.1) 20px 20px,rgba(49,112,45,0.05) 25px 25px;margin-bottom:4rem;padding:1rem}.session{display:flex;align-items:center;border-bottom:0 solid #eee;padding-left:1rem;padding-top:10px;padding-bottom:10px}.time{display:flex;justify-content:center;align-items:center;flex:0 0 20%;font-weight:600;color:#1f8f18;margin-right:1rem}.details{flex:1}.title{font-size:18px;margin-bottom:5px;font-weight:600}.speaker{font-size:16px;color:#666}.speaker-container{display:flex;flex-wrap:wrap}.speaker-profile{align-items:flex-start;gap:1rem;margin-bottom:2rem}.speaker-photo{width:12rem;object-fit:cover;min-height:10rem;max-height:13rem;margin-right:2rem;margin-top:7px;margin-bottom:1rem;float:left}.speaker-content{flex:1}.speaker-name{font-weight:bold;font-size:larger}.speaker-title{font-size:16px;color:#666}.speaker-link a{color:#1f8f18;text-decoration:none}.speaker-abstract{font-style:normal}.speaker-bio{font-weight:normal}.conference-materials{margin:2rem auto}.material-detail{background-color:rgba(49,112,45,0.04);margin-bottom:1rem;transition:all .3s ease-in-out}.material-summary{font-weight:600;color:#1f8f18;padding:1rem;margin:0;cursor:pointer;outline:0;position:relative;list-style:none;transition:all .3s ease-in-out}.material-summary::after{content:'▼';position:absolute;right:1rem;transition:transform .3s ease-in-out}.material-detail[open] .material-summary::after{transform:rotate(180deg)}.material-content{padding:1rem;line-height:1.6;max-height:0;overflow:overlay;transition:max-height .3s ease-in-out}.material-content b{font-weight:600}s .material-content ol{padding-left:10px;list-style-type:none;margin-top:1rem}.material-content a{color:#1f8f18;text-decoration:none}.material-content ol li{position:relative;padding-left:20px;margin-bottom:10px;margin-top:1rem}.material-content ol li::before{content:"•";color:#1f8f18;font-weight:bold;position:absolute;left:0;top:0;font-size:20px}.material-content-name{font-weight:600;font-size:initial;margin-top:-1rem;margin-bottom:1rem}.material-detail[open] .material-content{max-height:30rem}.material-summary:hover{background-color:var(--global-hover-colorr)}details>summary{list-style:none}details>summary::-webkit-details-marker{display:none}.logos-container{display:flex;justify-content:center;align-items:center;gap:20px;margin-top:1rem;flex-wrap:wrap}.logos-container img{max-height:min(7rem,18vw);width:auto;transition:transform .3s ease}.logos-container img:hover{transform:scale(1.1)}.special-logo-1{max-width:max(5rem,7vw)}.special-logo-2{max-width:max(6rem,8vw)}.foot-title{font-size:24px;color:#666;font-weight:700}.foot-link a{color:#1f8f18;text-decoration:none;font-size:24px;font-weight:300}.foot-title b{font-size:32px;font-weight:700}.register-button{display:inline-block;color:white!important;padding:10px 20px;text-align:center;text-decoration:none!important;font-size:21px;transition:all .3s ease;position:relative;overflow:hidden;margin-bottom:1rem;border-radius:5px;background:rgba(49,112,45,0.6)}.register-button::after{content:'→';position:absolute;right:-20px;top:50%;transform:translateY(-50%);transition:right .3s ease}
.register-button:hover::after{right:10px}.register-button:hover{padding-right:40px;box-shadow:rgba(49,112,45,0.4) 5px 5px,rgba(49,112,45,0.3) 10px 10px,rgba(49,112,45,0.2) 15px 15px,rgba(49,112,45,0.1) 20px 20px,rgba(49,112,45,0.05) 25px 25px}.modal{display:none;position:fixed;z-index:1;left:0;top:0;width:100%;height:100%;overflow:auto;background-color:rgba(0,0,0,0.4)}.modal-content{background-color:#fefefe;position:absolute;top:50%;left:50%;transform:translate(-50%,-50%);padding:20px;border:1px solid #888;width:90%;max-width:1200px;height:85%;overflow:auto;box-shadow:0 5px 15px rgba(0,0,0,0.3);border-radius:8px}.close{color:#aaa;float:right;font-size:28px;font-weight:bold}.close:hover,.close:focus{color:black;text-decoration:none;cursor:pointer}.nav-link{display:inline-block;font-size:16px;font-weight:bold;color:#fff;text-decoration:none;background-color:rgba(255,255,255,0.3);padding:8px 12px;border-radius:5px;backdrop-filter:blur(10px);box-shadow:0 2px 4px rgba(0,0,0,0.1);transition:background-color .3s,color .3s}.nav-link:hover{background-color:rgba(0,0,0,0.5);color:#fff}</style> <div class="container mx-auto px-4"> <div class="header-container"> <div class="title-container"> <h1 class="h1_1 text-6xl font-bold mt-10 mb-5 "> Workshop on</h1> <h2 class="h2_1 text-5xl font-bold mb-5">Frontiers of Image Science and Visual Computing 2025</h2> </div> <div class="image-container"> <img src="/assets/img/logos/University_of_Hong_Kong_Logo.png" alt="HKU" class="conference-image"> </div> </div> <div class="bg-gray-200 p-5 mb-5 conference-background"> <div class="text-container"> <h3 class="text-3xl font-bold mb-2 textareahead">Let there be light</h3> <p class="mb-2 textarea">15-16 April, 2025</p> <p class="mb-2 textarea">TBD, The University of Hong Kong, HONG KONG SAR</p> <a href="#" class="mb-2 nav-link" onclick="openModal()">Navigate Here</a> </div> </div> <div id="modal" class="modal"> <div class="modal-content"> <span class="close" onclick="closeModal()">×</span> <iframe id="modal-iframe" src="https://innowings.engg.hku.hk/innowing2/visitors/" width="100%" height="100%"></iframe> </div> </div> <h2 class="text-4xl font-bold mb-5">Workshop Objectives</h2> <div class="grid grid-cols-1 md:grid-cols-2 mb-10"> <div> <div class="flex items-start mb-2" style="margin-bottom: 2rem !important"> <div class="text-red-500 mr-2"> <i class="fas fa-circle"></i> </div> <div> <h4 class="font-bold">Encourage Innovative Spirit</h4> </div> </div> <div class="flex items-start mb-2" style="margin-bottom: 2rem !important"> <div class="text-red-500 mr-2"> <i class="fas fa-circle"></i> </div> <div> <h4 class="font-bold ">Promote Excellence and Sustain Quality</h4> </div> </div> </div> <div> <div class="flex items-start mb-2" style="margin-bottom: 2rem !important"> <div class="text-red-500 mr-2"> <i class="fas fa-circle"></i> </div> <div> <h4 class="font-bold ">Strive for Improvement</h4> </div> </div> <div class="flex items-start" style="margin-bottom: 2rem !important"> <div class="text-red-500 mr-2"> <i class="fas fa-circle"></i> </div> <div> <h4 class="font-bold ">Connect Communities</h4> </div> </div> </div> </div> <h2 class="text-4xl font-bold mb-5" style="margin-top: -2rem !important">Schedule</h2> <div class="schedule" id="schedule"></div> <script>function openModal(){document.getElementById("modal").style.display="flex"}function closeModal(){document.getElementById("modal").style.display="none"}const sessions=[{title:"Tuesday, 15 April",speaker:'MC: <a href="https://www.eee.hku.hk/~evanpeng/" target="_blank">Evan Y. Peng, HKU</a>',showTime:!1},{title:"Registration",duration:30,showTime:!0},{title:"Opening Remarks",duration:10,speaker:"TBD",showTime:!0},{title:"Keynote1",duration:50,speaker:"David Forsyth, UIUC",showTime:!0},{title:"Diffusion Posterior Sampling for Computational Imaging with Inaccurate Priors or Physics",duration:25,speaker:"He Sun, PKU",showTime:!0},{title:"Learning 3D Representations from Videos",duration:25,speaker:"Xiaojuan Qi, HKU",showTime:!0},{title:"ACM SIGGRAPH Asia 2025 Pitching",duration:5,speaker:"Taku Komura, HKU",showTime:!0},{title:"Group Photo",duration:5,showTime:!0},{title:"Lunch",duration:90,showTime:!0},{title:"Security Concerns on Visual AI beyond RGB Domain",duration:30,speaker:"Yinqiang Zheng, UTokyo",showTime:!0},{title:"Differentiable Acquisition of Appearance and Geometry",duration:25,speaker:"Hongzhi Wu, ZJU",showTime:!0},{title:"Talk4",duration:25,speaker:"Tianfan Xue, CUHK",showTime:!0},{title:"Tea Reception",duration:20,showTime:!0},{title:"Flat-Optics for Imaging with Extended Information",duration:25,speaker:"Yuanmu Yang, THU",showTime:!0},{title:"Talk6",duration:25,speaker:"Ping Tan, HKUST",showTime:!0},{title:"Computational Imaging for Graphics, Vision, Robotics, and Science",duration:25,speaker:"Seung-Hwan Baek, POSTECH",showTime:!0},{title:"Closing Remarks & Networking",duration:35,speaker:"TBD",showTime:!0},{title:"Dinner Banquet and HK-Night Tour",Time:"18:30",duration:150,speaker:"invitation only",showTime:!0},{title:"Wednesday, 16 April",showTime:!1},{title:"Fireside Chat (Research) and Lab Tour",Time:"10:00",duration:90,speaker:"invitation only",showTime:!0},{title:"Lunch-on Discussion and Sharing",duration:60,speaker:"invitation only",showTime:!0}];let currentTime=new Date;currentTime.setHours(10,0);const scheduleContainer=document.getElementById("schedule");scheduleContainer?sessions.forEach(e=>{const i=document.createElement("div");i.className="session";let t="";if(e.hasOwnProperty("Data"))t=e.Data;else if(e.hasOwnProperty("Time")&&e.hasOwnProperty("duration")){const[i,n]=e.Time.split(":").map(Number);currentTime.setHours(i,n);const o=new Date(currentTime.getTime()+6e4*e.duration);t=`${currentTime.toTimeString().substring(0,5)} - ${o.toTimeString().substring(0,5)}`,currentTime=o}else if(e.hasOwnProperty("duration")){const i=new Date(currentTime.getTime()+6e4*e.duration);t=`${currentTime.toTimeString().substring(0,5)} - ${i.toTimeString().substring(0,5)}`,currentTime=i}i.innerHTML=`\n          <div class="time">${e.showTime?t:""}</div>\n          <div class="details">\n            <div class="title" ${e.showTime?"":'style="color: #1f8f18 !important"'}>${e.title}</div>\n            ${e.speaker?`<div class="speaker">${e.speaker}</div>`:""}\n          </div>\n        `,scheduleContainer.appendChild(i)}):console.error("Schedule container not found"),window.onclick=function(e){const i=document.getElementById("modal");e.target==i&&closeModal()};</script> <h2 class="text-4xl font-bold mb-5">Speakers/Guests</h2> <div class="speaker-container"> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/David%20Forsyth.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">David Forsyth</h2> <h3 class="speaker-title">ACM Fellow, IEEE Fellow, University of Illinois Urbana-Champaign (UIUC)</h3> <p class="speaker-link"><a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> David Alexander Forsyth currently serves as a Professor at University of Illinois Urbana-Champaign. He previously held a professorial position at the University of California, Berkeley. His research expertise includes computer vision, computer graphics, and machine learning, and he has authored over 100 scholarly papers in these fields. He co-authored the influential textbook "Computer Vision: A Modern Approach" (Prentice-Hall, 2002) alongside J. Ponce. Prof. Forsyth earned his B.Sc. and M.Sc. degrees in Electrical Engineering from the University of the Witwatersrand in Johannesburg, South Africa, followed by a D.Phil. from Balliol College, Oxford, UK. In his academic leadership roles, he has served as a program co-chair for CVPR and ECCV. His contributions to the field were recognized with the IEEE Technical Achievement Award in 2005. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Yinqiang%20Zheng.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Yinqiang Zheng</h2> <h3 class="speaker-title">The University of Tokyo (UTokyo)</h3> <p class="speaker-link"><a href="https://researchmap.jp/yinqiangzheng?lang=en" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Dr. Yinqiang Zheng received his Doctoral degree of engineering from the Department of Mechanical and Control Engineering, Tokyo Institute of Technology, Tokyo, Japan, in 2013. He is currently a Full Professor in the Next Generation Artificial Intelligence Research Center, The University of Tokyo, Japan, leading the Optical Sensing and Camera System Laboratory (OSCARS Lab.). He has published a serial of research papers that bridge AI and optical imaging, surrounding the novel paradigms of ‘Optics for Better AI’ and ‘AI for Best Optics’. He has served as area chair for CVPR, ICCV, ICML, ICLR, NeurIPS, MM, 3DV, ACCV, ISAIR, DICTA and MVA. He is a foreign fellow of the Engineering Academy of Japan, and the recipient of the Konica Minolta Image Science Award and Funai Academic Award. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Seung-Hwan%20Baek.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Seung-Hwan Baek </h2> <h3 class="speaker-title"> Pohang University of Science and Technology (POSTECH)</h3> <p class="speaker-link"><a href="https://www.shbaek.com/home" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Seung-Hwan Baek is an Assistant Professor of Computer Science and Engineering at POSTECH, also affiliated with the Graduate School of AI, POSTECH. He leads POSTECH Computational Imaging Group and serves as a faculty co-director of POSTECH Computer Graphics Lab. Prof. Baek worked as a postdoctoral researcher at Princeton University, and obtained his Ph.D. in Computer Science from KAIST. Prof. Baek's research, situated at the intersection of computer graphics, vision, AI, and optics, focuses on capturing, modeling, and analyzing high-dimensional visual data originating from complex interplays between light, material appearance, and geometry. Prof. Baek has received awards, including Frontiers of Science Award from the International Congress of Basic Science, Outstanding Ph.D. Thesis Award in IT from the Korean Academy of Science and Technology, SIGGRAPH Asia Doctoral Consortium, Microsoft Research Asia Ph.D. Fellowship, and both ACCV Best Application Paper Award and Best Demo Award. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Yuanmu%20Yang.png" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Yuanmu Yang</h2> <h3 class="speaker-title">Tsinghua University (THU)</h3> <p class="speaker-link"><a href="https://ymyanggroup.com/people/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Yuanmu Yang is an Associate Professor in the Department of Precision Instrument at Tsinghua University. He earned his B.Eng. in Optoelectronics from Tianjin University and Nankai University in 2011 and his Ph.D. in Interdisciplinary Materials Science from Vanderbilt University in 2015. He was a postdoctoral researcher at Sandia National Laboratories, USA from 2015 to 2017. He then worked at Intellectual Ventures, a Seattle-based startup incubator, from 2017 to 2018, and was a founding team member of metasurface-based solid-state lidar startup Lumotive. His research focuses on the area of meta-optics. He has published more than 50 journal articles, including 2 in Nature Photonics and 1 in Nature Physics, received over 6000 citations according to Google Scholar, and was selected as a “Highly-cited Researcher in China” by Elsevier in 2023. He has been granted over 10 China and US patents. His recognitions include the Jin-Guofan Young Scientist Award given by the China Instrument and Control Society as well as the Forbes China “30 under 30” in 2018. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/He%20Sun.png" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">He Sun</h2> <h3 class="speaker-title">Peking University (PKU)</h3> <p class="speaker-link"><a href="https://ai4scientificimaging.org/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Dr. He Sun is an Assistant Professor in the National Biomedical Imaging Center, Peking University, China. Prior to starting at Peking University, he was a postdoctoral researcher in the Department of Computing and Mathematical Sciences at California Institute of Technology. He received his Ph.D. in Mechanical and Aerospace Engineering from Princeton University in 2019 and his bachelor's degree in Engineering Mechanics and Economics from Peking University in 2014. His research primarily focuses on computational imaging, which tightly integrates optics, control, signal processing and machine learning to push the boundary of scientific imaging. His past work has contributed to multiple real science missions, including the Event Horizon Telescope for black hole interferometric imaging. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Hongzhi%20Wu.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Hongzhi Wu</h2> <h3 class="speaker-title">Zhejiang University (ZJU)</h3> <p class="speaker-link"><a href="https://svbrdf.github.io/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Hongzhi Wu is a Full Professor in the State Key Lab of CAD&amp;CG, Zhejiang University, China. He obtained his Ph.D. from Yale University, under the supervision of Julie Dorsey and Holly Rushmeier, and was awarded Excellent Young Scholar by NSF China. His research focuses on the acquisition and reconstruction of physical information, including complex high-dimensional appearance, 3D surfaces and volumes. Hongzhi's research leads to publications in top venues including SIGGRAPH/SIGGRAPH Asia, CVPR and ICCV, two published books, as well as a number of state-of-the-art high-performance illumination multiplexing devices. He was the recipient of Luzengyong CAD&amp;CG High Tech Award (1st prize). Hongzhi is on the editorial board of IEEE TVCG, JCAD &amp; VCIBA, and has served on the program committees of various international conferences, including SIGGRAPH, SIGGRAPH Asia, VR, EG, PG, EGSR, I3D and HPG. More details can be found from his homepage https://svbrdf.github.io/. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Ping%20Tan.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Ping Tan</h2> <h3 class="speaker-title">Hong Kong University of Science and Technology (HKUST)</h3> <p class="speaker-link"><a href="https://pingtan.people.ust.hk/index.html" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Dr. Tan is a Professor in the Department of Electronic and Computer Engineering at the Hong Kong University of Science and Technology (HKUST). Before joining HKUST, he served as the director of the XR Lab at Alibaba DAMO Academy from 2019 to 2022, an Associate Professor at Simon Fraser University (SFU) in Canada from 2014 to 2019, and an assistant and Associate Professor at the National University of Singapore (NUS) from 2007 to 2014. Dr. Tan received his PhD from HKUST in 2007 and his Master's and Bachelor's degrees from Shanghai Jiao Tong University (SJTU) in 2003 and 2000, respectively. He specializes in computer vision, computer graphics, and robotics, with a research focus on 3D vision. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Tianfan%20Xue.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Tianfan Xue</h2> <h3 class="speaker-title">The Chinese University of Hong Kong (CUHK)</h3> <p class="speaker-link"><a href="https://tianfan.info/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Tianfan Xue is a Vice Chancellor Assistant Professor at the Department of Information Engineering, the Chinese University of Hong Kong (CUHK). Before joining CUHK, he worked in Google Research from 2017 to 2022. Prof. Xue received his Ph.D. from Massachusetts Institute of Technology (MIT) in 2017, his M.Phil. from CUHK in 2011, and his B.E. from Tsinghua University (THU) in 2009. Prof. Xue has published over 40 papers on top-tier journals and conferences, and he has served as an area chair for CVPR and WACV. His research interests include computational photography, computer vision, computer graphics, and machine learning. </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Xiaojuan%20Qi.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Xiaojuan Qi</h2> <h3 class="speaker-title">The University of Hong Kong (HKU)</h3> <p class="speaker-link"><a href="https://xjqi.github.io/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Xiaojuan Qi is currently an Assistant Professor in the Department of Electrical and Electronic Engineering, the University of Hong Kong (HKU). Before joining HKU, she was a postdoctoral researcher at the University of Oxford, UK. She received her Ph.D. from the Chinese University of Hong Kong (CUHK) in 2018 and her B.Eng. from Shanghai Jiao Tong University (SJTU) in 2014. From September 2016 to November 2016, she was a visiting student in the Machine Learning Group, University of Toronto. She has carried out an internship at Intel Intelligent Systems Lab from May 2017 to November 2017. She has won several awards such as the first place of ImageNet Semantic Parsing Challenge, Outstanding Reviewer in ICCV’17 and ICCV’19, CVPR’18 Doctoral Consortium Travel Award and Hong Kong PhD Fellowship (2014 – 2018). </p> </div> </div> <div class="speaker-profile"> <img class="speaker-photo" src="/assets/img/workshop/Taku%20Komura.jpg" alt="Speaker Name"> <div class="speaker-content"> <h2 class="speaker-name">Taku Komura</h2> <h3 class="speaker-title">General Chair of ACM SIGGRAPH Asia 2025, The University of Hong Kong (HKU)</h3> <p class="speaker-link"><a href="https://i.cs.hku.hk/~taku/" target="_blank" rel="external nofollow noopener">Personal Website</a></p> <p class="speaker-abstract"> Taku Komura joined the University of Hong Kong in 2020. Before joining HKU, he worked at the University of Edinburgh (2006-2020), City University of Hong Kong (2002-2006) and RIKEN (2000-2002). He received his BSc, MSc and PhD in Information Science from University of Tokyo. His research has focused on data-driven character animation, physically-based character animation, crowd simulation, 3D modelling, cloth animation, anatomy-based modelling and robotics. Recently, his main research interests have been on physically-based animation and the application of machine learning techniques for animation synthesis. He received the Royal Society Industry Fellowship (2014) and the Google AR/VR Research Award (2017). </p> </div> </div> </div> <div class="conference-materials"> <h2 class="text-4xl font-bold mb-5">Workshop Materials</h2> <details class="material-detail"> <summary class="material-summary">Keynote1 <span style="color:black;"> - David Forsyth</span></summary> <div class="material-content"> <p> TBD </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Diffusion Posterior Sampling for Computational Imaging with Inaccurate Priors or Physics <span style="color:black;"> - He Sun</span> </summary> <div class="material-content"> <p> Diffusion models excel in solving imaging inverse problems due to their exceptional ability to model complex image priors. When integrated with image formation models, they enable a physics-guided diffusion process—termed diffusion posterior sampling (DPS)—that effectively retrieves high-dimensional posterior distributions of images from corrupted observations. However, DPS’s dependence on clean diffusion priors and accurate imaging physics limits its practical utility, especially when clean data is scarce for pre-training diffusion models or when the underlying physical models are inaccurate for inference. However, DPS requires clean diffusion priors and accurate imaging physics for inference, limiting its practical use when clean data is scarce for pre-training diffusion models or when the underlying physical models are inaccurate or unknown. In this talk, Prof. Sun will introduce an expectation-maximization (EM) framework that adapts DPS for scenarios with inaccurate priors or physics. This method alternates between an E-step, in which clean images are reconstructed from corrupted data assuming known diffusion prior and physical model, and an M-step, where these reconstructions are employed to refine and recalibrate the prior or physical model. This iterative process progressively aligns the learned image prior with the true clean distribution and adapts the physical model for enhanced accuracy. They validate their approach through extensive experiments across diverse computational imaging tasks—including inpainting, deblurring, denoising, and realistic microscopic imaging—demonstrating new state-of-the-art performance. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Learning 3D Representations from Videos <span style="color:black;"> - Xiaojuan Qi</span></summary> <div class="material-content"> <p> Humans navigate a rich 3D world, continuously acquiring diverse skills and engaging in various activities through perception, understanding, and interaction. The long-term research goal of Prof. Qi's group is to simulate the dynamic 3D world and equip AI systems with 3D spatial understanding. A fundamental challenge in achieving this lies in how to effectively represent 3D structures. In this talk, Prof. Qi will present a series of research efforts from her group focused on learning 3D representations from videos for surface reconstruction, novel view synthesis, and 4D modeling of dynamic scenes, with applications in video processing. She will also discuss future directions toward generating editable 4D worlds from casually captured videos. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Security Concerns on Visual AI beyond RGB Domain <span style="color:black;"> - Yinqiang Zheng</span></summary> <div class="material-content"> <p> AI algorithms for computer-based visual understanding have advanced significantly, due to the prevalence of deep learning and large-scale visual datasets in the RGB domain, which have also been proven vulnerable to digital and physical adversarial attacks. To deal with complex scenarios, many other imaging modalities beyond the visibility scope of human eyes, such as near infrared (NIR), short wavelength infrared (SWIR), thermal infrared (TIR), polarization, neuromorphic pulse, have been introduced, yet the vulnerabilities of visual AI based on these non-RGB modalities have not received due attention. In this talk, Prof. Zheng will show that typical AI algorithms, like object detection and segmentation, can be more fragile than in the RGB domain, and the properly crafted attackers can hardly be noticed by naked human eyes. They showcase a serial of physics-based attackers in the NIR, SWIR, and TIR domain, a printable attacker for event-based human detection, and a projection-based attacker on polarization-based reconstruction and segmentation. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Differentiable Acquisition of Appearance and Geometry <span style="color:black;"> - Hongzhi Wu</span></summary> <div class="material-content"> <p> High-quality, efficient acquisition of complex appearance and geometry is a classic problem in computer graphics and vision. Differentiable acquisition maps both physical acquisition and computational reconstruction to a differentiable pipeline, enabling a fully automatic, joint optimization of hardware and software. This significantly improves modeling efficiency and quality over existing work. In this talk, Prof. Wu will briefly introduce his recent research on differential acquisition, including OpenSVBRDF, the first large-scale measured SVBRDF database (SIGAsia 2023a), the first dynamic visible-light tomography system (SIGAsia 2023b), as well as a system for real-time acquisition and reconstruction of dynamic 3D volumes (CVPR 2024). He will also describe on-going research on building a dedicated high-performance device for acquiring the appearance of cultural relics in the National Museum of China. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Talk4 <span style="color:black;"> - Tianfan Xue</span></summary> <div class="material-content"> <p> TBD </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Flat-Optics for Imaging with Extended Information <span style="color:black;"> - Yuanmu Yang</span></summary> <div class="material-content"> <p> Conventional cameras can only acquire light intensity in two dimensions. In order to further obtain the depth, polarization, and spectral information of the target object or to achieve imaging resolution beyond the diffraction limit, it is often required to use bulky and expensive instruments. Here, Prof. Yang will present his group’s recent effort to replace conventional camera lenses with flat diffractive optical elements or metasurfaces. By leveraging the unique capability of metasurface to tailor the vectorial field of light, in combination with an advanced image retrieval algorithm, they aim to build compact camera systems that can capture multi-dimensional light field information of a target scene in a single shot under ambient illumination conditions. Specifically, he will show how they use flat-optics to build a monocular camera that can capture a 4D image, including 2D all-in-focus intensity, depth, and polarization of a target scene in a single shot over extended scenes. He would also like to present their effort to construct flat-optics-based monocular 3D camera modules for real-world applications. </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Talk6 <span style="color:black;"> - Ping Tan</span></summary> <div class="material-content"> <p> TBD </p> </div> </details> <details class="material-detail"> <summary class="material-summary">Computational Imaging for Graphics, Vision, Robotics, and Science <span style="color:black;"> - Seung-Hwan Baek</span></summary> <div class="material-content"> <p> In this talk, Prof. Baek will discuss his recent work on designing, controlling, and utilizing computational imaging systems for inverse rendering, robotic vision in extreme conditions, transparent and metameric material analysis, and advanced VR/AR systems. The core idea is to take a holistic approach—considering rendering, the transformation of multi-dimensional light properties as they travel from the source through a scene, their interaction with imaging systems, and the computational processes for reconstruction and restoration. </p> </div> </details> </div> <footer style="text-align:center; padding:20px;"> <p class="foot-title">Organizer: <span class="foot-link"><a href="https://hku.welight.fun/">Computational Imaging &amp; Mixed Representation Laboratory</a></span></p> <p class="foot-title">Local Coordinating Team: <span class="foot-link"><a href="https://andrewbxy.github.io/" rel="external nofollow noopener" target="_blank">Xiaoyang Bai, </a><a href="https://liux2018.github.io/" rel="external nofollow noopener" target="_blank">Xin Liu, </a><a href="https://qufeifan.github.io/" rel="external nofollow noopener" target="_blank">Feifan Qu, </a><a href="https://liangxunou.github.io/" rel="external nofollow noopener" target="_blank">Liangxun Ou</a></span></p> <p class="foot-title">Advisory Committee: <span class="foot-link"><a href="https://vccimaging.org/People/heidriw/" rel="external nofollow noopener" target="_blank">Wolfgang Heidrich, HKU/KAUST &amp; </a><a href="https://www.eee.hku.hk/~huangkb/" rel="external nofollow noopener" target="_blank">Kaibin Huang, HKU</a></span></p> <p class="foot-title">Chair: <span class="foot-link"><a href="https://www.eee.hku.hk/~evanpeng/" rel="external nofollow noopener" target="_blank">Evan Y. Peng, HKU EEE x CS</a></span></p> <br> <p class="foot-title "><b>Acknowledgement: </b></p> <div class="logos-container"> <a href="https://www.hku.hk" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/HKU.png" alt="Organization Logo"> </a> <a href="https://illinois.edu/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/UIUC.png" alt="Organization Logo"> </a> <a href="https://www.postech.ac.kr/eng/index.do" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/POSTECH.png" alt="Organization Logo"> </a> <a href="https://www.u-tokyo.ac.jp/en/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/UTokyo.png" alt="Organization Logo"> </a> <a href="https://www.tsinghua.edu.cn/en/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/THU.png" alt="Organization Logo"> </a> <a href="https://english.pku.edu.cn/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/PKU.png" alt="Organization Logo"> </a> <a href="https://www.zju.edu.cn/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/ZJU.png" alt="Organization Logo"> </a> <a href="https://hkust.edu.hk/" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/HKUST.png" alt="Organization Logo"> </a> <a href="https://www.cuhk.edu.hk/english/index.html" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/logos/CUHK.png" alt="Organization Logo" class="special-logo-2"> </a> </div> </footer> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>